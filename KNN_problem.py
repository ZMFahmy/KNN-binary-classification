# -*- coding: utf-8 -*-
"""KNN_problem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1THB3QAxra1l-012mUTJ6C1HOElUouRn_

## Importing libraries
"""

import numpy as np
import pandas as pd

"""## Preparing arrays of dependent and independent variables"""

dataset = pd.read_csv('magic04.data')
X = dataset.iloc[:, :-1].values # Matrix of features
y = dataset.iloc[:, -1].values # Vector of predicted values

print(X)

print(y)

"""## Balancing out the data"""

# We count the appearance of each class in our dataset

g_counter = 0
h_counter = 0

for c in y:
  if c == 'g':
    g_counter += 1
  else:
    h_counter += 1

print(g_counter)

print(h_counter)

# We balance the dataset by randomly removing extra data points
# that result in the g class until both classes are approximately equal

i = 0
indices = []

while g_counter > h_counter:
  if y[i] == 'g':
    indices.append(i)
    g_counter -= 1
  i += 1

# We remove the same rows from both X and y

X = np.delete(X, indices, axis=0)
y = np.delete(y, indices, axis=0)

"""## Splitting the data to training, validation, and test sets"""

from sklearn.model_selection import train_test_split

# training set will be 70% of dataset
X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.3)

# the remaining 30% is divided equally among validation and test sets
X_validation, X_test, y_validation, y_test = train_test_split(X_rest, y_rest, test_size=0.5)

"""## Performing feature scaling"""

# Performing feature scaling is necessary, so that no features dominate
# the model due to their large range

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_validation = sc.transform(X_validation)
X_test = sc.transform(X_test)

print(X_train)

"""## Training model on training set"""

from sklearn.neighbors import KNeighborsClassifier

# Standard is using minkowski metric, which uses manhattan distance if
# p=1, and uses euclidean distance if p=2
classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)

# Training model on training set
classifier.fit(X_train, y_train)

"""## Choosing the best k value to use"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

suggested_neighbors = range(1, 21)

# We will store these metrics for analysing performance the of model with
# different K values later

accuracy_scores_on_validation = []
precision_scores_on_validation = []
recall_scores_on_validation = []
f_scores_on_validation = []
confusion_matrices_on_validation = []

accuracy_scores_on_test = []
precision_scores_on_test = []
recall_scores_on_test = []
f_scores_on_test = []
confusion_matrices_on_test = []

# Getting scores by comparing predictions and true values from validation set
for k in suggested_neighbors:
  classifier.n_neighbors = k
  y_pred = classifier.predict(X_validation)

  accuracy_scores_on_validation.append(accuracy_score(y_validation, y_pred))

  precision = precision_score(y_validation, y_pred, pos_label='g')
  precision_scores_on_validation.append(precision)

  recall = recall_score(y_validation, y_pred, pos_label='g')
  recall_scores_on_validation.append(recall)

  f_scores_on_validation.append(2 * (precision * recall) / (precision + recall))

  confusion_matrices_on_validation.append(confusion_matrix(y_validation, y_pred))

# Getting scores by comparing predictions and true values from test set
for k in suggested_neighbors:
  classifier.n_neighbors = k
  y_pred = classifier.predict(X_test)

  accuracy_scores_on_test.append(accuracy_score(y_test, y_pred))

  precision = precision_score(y_test, y_pred, pos_label='g')
  precision_scores_on_test.append(precision)

  recall = recall_score(y_test, y_pred, pos_label='g')
  recall_scores_on_test.append(recall)

  f_scores_on_test.append(2 * (precision * recall) / (precision + recall))

  confusion_matrices_on_test.append(confusion_matrix(y_test, y_pred))

# Selecting most efficient K
best_k = pd.Series(accuracy_scores_on_validation).idxmax() + 1
classifier.n_neighbors = best_k

print(f"Hypothetically, model works best at n_neighbors = {best_k}")

"""## Comparing performance metrics for different models"""

from prettytable import PrettyTable, ALL
report_table_on_validation = PrettyTable(["K value", "Accuracy", "Precision", "Recall", "F-score", "Confusion matrix"])
report_table_on_validation.hrules = ALL

report_table_on_test = PrettyTable(["K value", "Accuracy", "Precision", "Recall", "F-score", "Confusion matrix"])
report_table_on_test.hrules = ALL

for i in range(0, 20):
  accuracy = float("{:.4f}".format(accuracy_scores_on_validation[i]))
  precision = float("{:.4f}".format(precision_scores_on_validation[i]))
  recall = float("{:.4f}".format(recall_scores_on_validation[i]))
  f_score = float("{:.4f}".format(f_scores_on_validation[i]))
  cm = confusion_matrices_on_validation[i]

  report_table_on_validation.add_row([i+1, accuracy, precision, recall, f_score, cm])

for i in range(0, 20):
  accuracy = float("{:.4f}".format(accuracy_scores_on_test[i]))
  precision = float("{:.4f}".format(precision_scores_on_test[i]))
  recall = float("{:.4f}".format(recall_scores_on_test[i]))
  f_score = float("{:.4f}".format(f_scores_on_test[i]))
  cm = confusion_matrices_on_test[i]

  report_table_on_test.add_row([i+1, accuracy, precision, recall, f_score, cm])



print("                       Results on Validation set")
print(report_table_on_validation)

print("\n\n                       Results on Test set")
print(report_table_on_test)

print(f"\nBest accuracy on test data = {max(accuracy_scores_on_test)}, using k = {pd.Series(accuracy_scores_on_validation).idxmax() + 1}")